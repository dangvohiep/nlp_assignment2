{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liqKR9Vk9RSj"
      },
      "source": [
        "## Assignment 3 Naïve Bayes and Sentiment Classification and Logistic Regression\n",
        "Instructions\n",
        "* Read the following Chapter 4: Naive Bayes and Sentiment Classification. Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021. I have tried to pull out relevant notes for you below, but it is encouraged that you read each chapter provided.\n",
        "* Read the following Chapter 5: Logistic Regression. Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021. I have tried to pull out relevant notes for you below, but it is encouraged that you read each chapter provided.\n",
        "\n",
        "Summary\n",
        "Classification is one of the most important tasks of NLP and in machine learning. In NLP it often means the task of text categorization for both sentiment analysis, spam detection, and topic modeling. Naïve Bayes is often one of the first classification algorithms defined in NLP.  The intuition behind a classifier is lies at the underlying probability inferred by the Bayesian Inference, which uses Baye’s rule and conditional probabilities.\n",
        "\n",
        "Here’s a reminder on Baye’s Rule:\n",
        "P(y)=P(x)P(x)/(P(y))\n",
        "\n",
        "We are saying “what is the probability of x given y”. Naïve Bayes is a generative model because there is an input that helps the model determine what the output could be. Said differently, “to train a generative model we first collect a large amount of data in some domain (e.g., think millions of images, sentences, or sounds, etc.) and then train a model to generate data like it.” [6]\n",
        "\n",
        "So in the case of Naïve Bayes, we say given some word, what should be the class of the current word we are assessing? Contrastingly, discriminative models such as logistic regression, learn from features provided to the algorithm and then determine or predict what the class is. [7]\n",
        "\n",
        "\n",
        "With Naïve Bayes, the assumption is that the probabilities are independent. We often call the Naïve Bayes classifier the bag-of-words approach. That’s because we are essentially throwing in the collection of words into a ‘bag’, selecting a word at random, and then calculating their frequency to use in the Bayesian Inference. Thus, context – the position of words -- is ignored and despite this, it turns out that the Naïve Bayes approach can be accurate and effective at determining whether an email is spam for example.\n",
        "\n",
        "Back to bag of words. With bag of words, we assume that the position of the words are not relevant -- that dependency or context in the word phrase or sentence doesn’t matter. Relatedly, the naive Bayes assumption implies that the conditional probabilities are independent -- a rather strange assumption to make for words in a sentence! The equation for the naive Bayes classifier is outlined below:\n",
        "\n",
        "You can use Naive Bayes by creating an index of words and walking through every word position in a test or corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIBB2IVT92Ed"
      },
      "source": [
        "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.\n",
        "\n",
        "For this Assignment, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   One example corpus:   https://spamassassin.apache.org/old/publiccorpus/\n",
        "\n",
        "You may work alone or in a group on this project.  You're welcome to use any tools or approach that you like.  Due before our next meetup. Starter code provided below.\n",
        "\n",
        "Test example is provided at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uObO057u-Rne"
      },
      "source": [
        "Download corpus using the following functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C4fIGS9-8wce"
      },
      "outputs": [],
      "source": [
        "from os import makedirs, path, rename\n",
        "from tarfile import open as open_tar\n",
        "from shutil import rmtree\n",
        "from urllib import request, parse\n",
        "from glob import glob\n",
        "from os import path\n",
        "from glob import glob\n",
        "\n",
        "def download_corpus(dataset_dir: str = 'data'):\n",
        "    base_url = 'https://spamassassin.apache.org'\n",
        "    corpus_path = 'old/publiccorpus'\n",
        "    files = {\n",
        "        '20021010_easy_ham.tar.bz2': 'ham',\n",
        "        '20021010_hard_ham.tar.bz2': 'ham',\n",
        "        '20021010_spam.tar.bz2': 'spam',\n",
        "        '20030228_easy_ham.tar.bz2': 'ham',\n",
        "        '20030228_easy_ham_2.tar.bz2': 'ham',\n",
        "        '20030228_hard_ham.tar.bz2': 'ham',\n",
        "        '20030228_spam.tar.bz2': 'spam',\n",
        "        '20030228_spam_2.tar.bz2': 'spam',\n",
        "        '20050311_spam_2.tar.bz2': 'spam' }\n",
        "\n",
        "    #creates the folders: downloads, ham and spam\n",
        "    downloads_dir = path.join(dataset_dir, 'downloads')\n",
        "    ham_dir = path.join(dataset_dir, 'ham')\n",
        "    spam_dir = path.join(dataset_dir, 'spam')\n",
        "\n",
        "    makedirs(downloads_dir, exist_ok=True)\n",
        "    makedirs(ham_dir, exist_ok=True)\n",
        "    makedirs(spam_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    for file, spam_or_ham in files.items():\n",
        "        # download files from URL of each specific .bz2 file\n",
        "        url = parse.urljoin(base_url, f'{corpus_path}/{file}')\n",
        "        tar_filename = path.join(downloads_dir, file)\n",
        "        request.urlretrieve(url, tar_filename)\n",
        "\n",
        "        #list e-mails in the compressed .bz2 file\n",
        "        emails = []\n",
        "        with open_tar(tar_filename) as tar:\n",
        "            tar.extractall(path=downloads_dir)\n",
        "            for tarinfo in tar:\n",
        "                if len(tarinfo.name.split('/')) > 1:\n",
        "                    emails.append(tarinfo.name)\n",
        "\n",
        "        # move e-mails to ham or spam directory\n",
        "        for email in emails:\n",
        "            directory, filename = email.split('/')\n",
        "            directory = path.join(downloads_dir, directory)\n",
        "\n",
        "            if not path.exists(path.join(dataset_dir, spam_or_ham, filename)):\n",
        "                rename(path.join(directory, filename),\n",
        "                   path.join(dataset_dir, spam_or_ham, filename))\n",
        "\n",
        "        rmtree(directory)\n",
        "\n",
        "download_corpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUmHvbCn-o3s"
      },
      "source": [
        "#How many e-mails are classified in our dataset as either Spam or not Spam?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cx-Blo33-oM1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Non-Spam E-mails: 6952\n",
            "\n",
            "Number of Spam E-mails: 2399\n"
          ]
        }
      ],
      "source": [
        "#How many e-mails are classified in our dataset as either Spam or not Spam?\n",
        "ham_dir = path.join('data', 'ham')\n",
        "spam_dir = path.join('data', 'spam')\n",
        "\n",
        "print('Number of Non-Spam E-mails:', len(glob(f'{ham_dir}/*')))\n",
        "print('\\nNumber of Spam E-mails:', len(glob(f'{spam_dir}/*')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3fSuJ0G_jNG"
      },
      "source": [
        "Provide your classifier below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MO9eKbq8_llJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/hiepdang/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9754142169962586\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98      1378\n",
            "           1       0.98      0.93      0.95       493\n",
            "\n",
            "    accuracy                           0.98      1871\n",
            "   macro avg       0.98      0.96      0.97      1871\n",
            "weighted avg       0.98      0.98      0.98      1871\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords and wordnet\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Data cleaning function\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    # Substitute multiple spaces with single space\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "    # Remove prefixed 'b'\n",
        "    text = re.sub(r'^b\\s+', '', text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "def load_data(ham_dir, spam_dir):\n",
        "    data = []\n",
        "    labels = []\n",
        "    \n",
        "    # Load ham emails\n",
        "    for filename in glob.glob(os.path.join(ham_dir, '*')):\n",
        "        with open(filename, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            data.append(clean_text(file.read()))\n",
        "            labels.append(0)  # 0 for ham\n",
        "    \n",
        "    # Load spam emails\n",
        "    for filename in glob.glob(os.path.join(spam_dir, '*')):\n",
        "        with open(filename, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            data.append(clean_text(file.read()))\n",
        "            labels.append(1)  # 1 for spam\n",
        "    \n",
        "    return data, labels\n",
        "\n",
        "# Load dataset\n",
        "ham_dir = 'data/ham'\n",
        "spam_dir = 'data/spam'\n",
        "data, labels = load_data(ham_dir, spam_dir)\n",
        "\n",
        "# Split dataset into training and testing set\n",
        "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction and classifier pipeline\n",
        "pipeline = make_pipeline(\n",
        "    CountVectorizer(),  # transforms raw text documents into a matrix of token counts (labels are ignored)\n",
        "    MultinomialNB(),    # fit a Naive Bayes Model on the count matrix\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(data_train, labels_train)\n",
        "\n",
        "# Predict on test data\n",
        "predictions = pipeline.predict(data_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Accuracy:\", accuracy_score(labels_test, predictions))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(labels_test, predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5WtArb0_IMk"
      },
      "source": [
        "The following email is a test email. You can take this and test your classifier to see if it predicts spam or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_lvIjkRW_O8e"
      },
      "outputs": [],
      "source": [
        "spam_email = \"\"\"\n",
        "Subject: Get Rich Quick!\n",
        "\n",
        "Dear Friend,\n",
        "\n",
        "Congratulations! You've been selected to participate in an exclusive opportunity to make thousands of dollars from the comfort of your own home. Our revolutionary system guarantees quick and easy cash with minimal effort.\n",
        "\n",
        "No more struggling to pay bills or worrying about financial security. With our proven method, you can start earning massive amounts of money in no time.\n",
        "\n",
        "Here's what some of our satisfied customers have to say:\n",
        "- \"I was skeptical at first, but I'm now living my dream life thanks to this incredible system!\" - John S.\n",
        "- \"I never thought making money online could be this simple. It's changed my life!\" - Sarah L.\n",
        "\n",
        "Don't miss out on this limited-time offer. Act now to secure your spot and start enjoying a life of financial freedom.\n",
        "\n",
        "Click the link below to get started:\n",
        "www.getrichquick.com\n",
        "\n",
        "Remember, this opportunity is exclusive and won't last long. Take control of your financial future today!\n",
        "\n",
        "Best regards,\n",
        "The Get Rich Quick Team\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This model predicts the email as: spam\n"
          ]
        }
      ],
      "source": [
        "label = pipeline.predict([clean_text(spam_email)]).item()\n",
        "print('This model predicts the email as:', 'spam' if label else 'ham')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
